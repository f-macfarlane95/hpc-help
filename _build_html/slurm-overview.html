<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Slurm - Overview &mdash; Crop Diversity HPC Help  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Slurm - Queue Policies &amp; Advice" href="slurm-policy.html" />
    <link rel="prev" title="Guest Accounts" href="guest-accounts.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Crop Diversity HPC Help
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="system-overview.html">System Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="user-accounts.html">Requesting an Account</a></li>
<li class="toctree-l1"><a class="reference internal" href="ssh.html">Getting Connected</a></li>
<li class="toctree-l1"><a class="reference internal" href="guest-accounts.html">Guest Accounts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High performance computing</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Slurm - Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#jobs">Jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#queues-partitions">Queues (partitions)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interactive-jobs">Interactive jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#batch-jobs">Batch jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#array-jobs">Array jobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Allocating resources</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#queues-cpus-and-memory">Queues, CPUs, and memory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu-resources">GPU resources</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cancelling-a-job">Cancelling a job</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scratch-space">Scratch space</a></li>
<li class="toctree-l2"><a class="reference internal" href="#job-summaries">Job summaries</a></li>
<li class="toctree-l2"><a class="reference internal" href="#other-helpful-parameters">Other helpful parameters</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#change-working-directory">Change working directory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#email-notifications">Email notifications</a></li>
<li class="toctree-l3"><a class="reference internal" href="#job-testing">Job testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#specifying-nodes">Specifying nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#submitting-binaries">Submitting binaries</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="slurm-policy.html">Slurm - Queue Policies &amp; Advice</a></li>
<li class="toctree-l1"><a class="reference internal" href="slurm-shortcuts.html">Slurm - Shortcuts and Aliases</a></li>
<li class="toctree-l1"><a class="reference internal" href="bioconda.html">Bioconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling.html">Compiling Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="file-compression.html">Working with Compressed Files</a></li>
<li class="toctree-l1"><a class="reference internal" href="singularity.html">Apptainer (Singularity)</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">GPU Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="machine-learning.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="apps.html">Tools &amp; Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="database-mirrors.html">Database Mirrors</a></li>
<li class="toctree-l1"><a class="reference internal" href="openmpi.html">OpenMPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="green-computing.html">Green Computing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data storage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data-storage.html">Data Storage &amp; Management Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="backups.html">Backups</a></li>
<li class="toctree-l1"><a class="reference internal" href="file-transfers.html">File Transfers</a></li>
<li class="toctree-l1"><a class="reference internal" href="samba.html">Samba Access</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contact-us.html">Contact Us</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">Monitoring &amp; Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="linux-basics.html">Linux Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips &amp; Tricks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">The legal stuff</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="organizations.html">Supported Organisations</a></li>
<li class="toctree-l1"><a class="reference internal" href="sla.html">Service Level Agreement</a></li>
<li class="toctree-l1"><a class="reference internal" href="acceptable-use.html">Acceptable Use Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="gdpr.html">GDPR Privacy Notice</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Crop Diversity HPC Help</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Slurm - Overview</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/cropgeeks/hpc-help/blob/master/slurm-overview.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="slurm-overview">
<h1>Slurm - Overview<a class="headerlink" href="#slurm-overview" title="Permalink to this headline"></a></h1>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Ideally, you’ll read <em>all</em> of this page before attempting to use the cluster, even if you’re already used to Slurm from other HPC setups, as the exact configuration and rules/policy almost always differ from system to system.</p>
</div>
<p>Cluster jobs are managed using the <strong>Slurm Workload Manager</strong> (<a class="reference external" href="https://slurm.schedmd.com/">https://slurm.schedmd.com/</a>).</p>
<p>Slurm is responsible for accepting, scheduling, dispatching, and managing the execution of jobs submitted to the cluster. At the most basic level, you put the commands you want to run into a script file, submit that script to the cluster’s queue (of jobs), and Slurm will then select a compute node for it to run on once it reaches the front of the queue. There is also the capability to run interactive sessions on a node.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>All data analysis programs and computational workloads <strong>must</strong> be run using Slurm. You should only use the head node (<code class="docutils literal notranslate"><span class="pre">gruffalo</span></code>) for tasks such as job submission and monitoring - it doesn’t have the resources available to support heavy workloads.</p>
</div>
<p>The Slurm man page is a useful starting point for understanding Slurm, viewable by running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ man slurm
</pre></div>
</div>
<p>Slurm commands (each with their own man pages) include tools such as <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> and <code class="docutils literal notranslate"><span class="pre">srun</span></code> to initiate jobs, <code class="docutils literal notranslate"><span class="pre">scancel</span></code> to terminate queued or running jobs, <code class="docutils literal notranslate"><span class="pre">sinfo</span></code> to report system status, and <code class="docutils literal notranslate"><span class="pre">squeue</span></code> to report the status of jobs. We’ll cover most of these in our guide below.</p>
<section id="jobs">
<h2>Jobs<a class="headerlink" href="#jobs" title="Permalink to this headline"></a></h2>
<p>Each element of work within Slurm is known as a <strong>job</strong> or job <strong>task</strong> (because some jobs can contain multiple tasks). A job includes a description of what to do, for example an executable command, and a set of parameter definitions that describe how the job should be run. Slurm locates an appropriate free resource - one or more compute nodes - on which to run the job and sends it there to be executed.</p>
<p>Slurm recognises four basic classes of jobs: <strong>interactive jobs</strong>, <strong>batch jobs</strong>, <strong>array jobs</strong>, and <strong>parallel jobs</strong>.</p>
<ul class="simple">
<li><p>An <a class="reference external" href="#interactive-jobs">interactive job</a> provides you with an interactive login to an available compute node in the cluster, allowing you to execute work that is not easily submitted as a batch job.</p></li>
<li><p>A traditional <a class="reference external" href="#batch-jobs">batch job</a> is single segment of work that is executed only once, running until it completes with no user interaction.</p></li>
<li><p>An <a class="reference external" href="#array-jobs">array job</a> consists of a series of tasks that can all be run in parallel but are completely independent of one another (eg compressing a set of files in a directory). All of the tasks of an array job are usually identical except for the data sets on which they operate.</p></li>
<li><p>A parallel job consists of a series of cooperating tasks that must all be executed at the same time, often with requirements about how the tasks are distributed across the resources. Very often parallel jobs make use of a parallel environment, such as MPI to enable the tasks to intercommunicate.</p></li>
</ul>
</section>
<section id="queues-partitions">
<h2>Queues (partitions)<a class="headerlink" href="#queues-partitions" title="Permalink to this headline"></a></h2>
<p>Jobs are scheduled by submitting them to a queue - or <strong>partition</strong> as Slurm likes to call them - and each queue has a number of available <strong>CPUs</strong> that can be used for running jobs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We have three main queues and three addition special queues (described in more detail on the <a class="reference internal" href="slurm-policy.html"><span class="doc">Slurm - Queue Policies &amp; Advice</span></a> page). If you don’t specify one specifically, then jobs are submitted to the <code class="docutils literal notranslate"><span class="pre">medium</span></code> queue by default.</p>
</div>
<p>You can see the status of the queue(s) at any time by using the <code class="docutils literal notranslate"><span class="pre">squeue</span></code> command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ squeue
     JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
     21356    medium    bash1   dvader  R       0:15      1 n19-32-192-hela
     21357    medium    bash2   dvader  R       0:15      1 n19-32-192-hela
     21358    medium    bash3   dvader  R       0:15      1 n19-32-192-hulk
</pre></div>
</div>
<p>This outputs a list of jobs, alongside information on the queue they’re running on, their name, state (eg, R=active, PD=waiting), owner, runtime, and the resources in use.</p>
<p>To identify what resources are currently available use the <code class="docutils literal notranslate"><span class="pre">sinfo</span> <span class="pre">-N</span></code> command, which will list the available compute nodes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sinfo -N
NODELIST                  NODES PARTITION STATE
...
n19-04-008-cortana            1     debug idle
n19-32-192-ghost              1     short idle
n19-32-192-groot              1     short idle
n19-32-192-hela               1   medium* idle
n19-32-192-hulk               1   medium* idle
n19-32-192-thor               1      long alloc
n19-32-192-ultron             1      long idle
...
</pre></div>
</div>
<p>The compute nodes are listed against the partition they can be accessed from and their current state: unused (<code class="docutils literal notranslate"><span class="pre">idle</span></code>), partially used by running jobs (<code class="docutils literal notranslate"><span class="pre">mix</span></code>) or completed occupied (<code class="docutils literal notranslate"><span class="pre">alloc</span></code>). An offline node will shown as <code class="docutils literal notranslate"><span class="pre">down</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The nodes are named in the form <code class="docutils literal notranslate"><span class="pre">nYY-CPU-MEM-name</span></code>, for example <code class="docutils literal notranslate"><span class="pre">n19-32-192-hulk</span></code> means it was purchased in 2019, has 32 CPU cores, 192 GB of memory and is named <code class="docutils literal notranslate"><span class="pre">hulk</span></code>. With the exception of <code class="docutils literal notranslate"><span class="pre">cortana</span></code>, all nodes have hyperthreading enabled, so their actual CPU count (as seen by Slurm) is doubled, meaning for example, that 64 ‘CPUs’ are available for use on <code class="docutils literal notranslate"><span class="pre">hulk</span></code>.</p>
</div>
<p>You can also get a graphical overview of the state of the cluster and see how busy/allocated it is by visiting <a class="reference external" href="https://www.cropdiversity.ac.uk/top/">https://www.cropdiversity.ac.uk/top/</a></p>
<blockquote>
<div><p><img alt="top" src="_images/top.png" /></p>
</div></blockquote>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>You should also make yourself aware of our <a class="reference internal" href="green-computing.html"><span class="doc">Green Computing</span></a> policy, that may keep idle nodes in a powered-down state until they are needed.</p>
</div>
</section>
<section id="interactive-jobs">
<h2>Interactive jobs<a class="headerlink" href="#interactive-jobs" title="Permalink to this headline"></a></h2>
<p>Starting an interactive job is a quick and easy way to get access to a powerful compute node, allowing you to start running analyses with the minimum of fuss. Use the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command, as follows, to start an interactive job:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ srun --pty bash
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can also use just <code class="docutils literal notranslate"><span class="pre">srsh</span></code>. See <a class="reference internal" href="slurm-shortcuts.html"><span class="doc">Slurm - Shortcuts and Aliases</span></a> for details.</p>
</div>
<p>This will assign you an interactive shell on an available node and reserve - by default - 1 CPU and 1.5 GB of memory for your use until you exit the shell. (Information on how to request more than the default is given below in the <a class="reference external" href="#id1">Allocating resources</a> section.)</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Remember that this allocation is reserved for your use for the entire time the shell is active, so if you are not doing anything you should really exit the shell - type <code class="docutils literal notranslate"><span class="pre">exit</span></code> or <code class="docutils literal notranslate"><span class="pre">CTRL+D</span></code> - to avoid tying up resources that could otherwise be utilized.</p>
</div>
<p>If you want to run an interactive, <em>graphical</em> job, then you can enable X11 forwarding as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ srun --x11 --pty bash
</pre></div>
</div>
<p>This will only work if you’ve got a local X-Server running and connected to <code class="docutils literal notranslate"><span class="pre">gruffalo</span></code> with X11 enabled (ie used <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-Y</span></code>). Note that performance over the internet with X11 is usually poor too.</p>
</section>
<section id="batch-jobs">
<h2>Batch jobs<a class="headerlink" href="#batch-jobs" title="Permalink to this headline"></a></h2>
<p>Most long running jobs should be handled using a job script, where you wrap the commands you want to run into a script file and then submit that. Here’s an example, showing the contents of a file called <code class="docutils literal notranslate"><span class="pre">test.sh</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --job-name=&quot;test job&quot;</span>
<span class="c1">#SBATCH --export=ALL</span>

<span class="n">echo</span> <span class="s2">&quot;Starting job on $HOSTNAME&quot;</span>
<span class="n">sleep</span> <span class="mi">60</span>
<span class="n">echo</span> <span class="s2">&quot;Job finished&quot;</span>
</pre></div>
</div>
<p>This is a normal bash shell script with some extra Slurm parameters (more on them later) inserted near the top using the <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code> prefix, in this case to say that we want the job to be called “test job” and we’d like to export all environment variables from the submitting shell into the job’s environment. The job in this case is a few simple steps to print out (echo) some infomation, along with a command to “sleep for 60 seconds”.</p>
<p>The job is submitted using <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sbatch test.sh
</pre></div>
</div>
<p>After submission, the job is assigned a unique ID and added to the queue, then run once a resource that can support it is available. We can check its status with <code class="docutils literal notranslate"><span class="pre">squeue</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ squeue
     JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
     21369    medium  testing   dvader  R       0:30      1 n19-32-192-hela
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Any output and error information that would normally have been printed to the screen are merged together into a file named using the job’s ID (<code class="docutils literal notranslate"><span class="pre">slurm-21369.out</span></code> in this example) but you can override this behaviour using the <code class="docutils literal notranslate"><span class="pre">--output</span></code> and <code class="docutils literal notranslate"><span class="pre">--error</span></code> parameters.</p>
</div>
</section>
<section id="array-jobs">
<h2>Array jobs<a class="headerlink" href="#array-jobs" title="Permalink to this headline"></a></h2>
<p>An array job is one in which the submitted script is run multiple times. The individual instances of the job, known as <strong>tasks</strong>, are distinguished by the value of the <code class="docutils literal notranslate"><span class="pre">$SLURM_ARRAY_TASK_ID</span></code> environment variable. For example, if an array job of 50 tasks is run as follows, <code class="docutils literal notranslate"><span class="pre">$SLURM_ARRAY_TASK_ID</span></code> will have a value of 1 in the first instance, 2 in the second instance, and so on up to 50.</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">--array</span></code> option to specify an array job, eg:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash

#SBATCH --array=1-50

mycommand.exe input_file_$SLURM_ARRAY_TASK_ID
</pre></div>
</div>
<p>This example will run <code class="docutils literal notranslate"><span class="pre">mycommand.exe</span></code> 50 times, starting with <code class="docutils literal notranslate"><span class="pre">input_file_1</span></code>, <code class="docutils literal notranslate"><span class="pre">input_file_2</span></code>, and so on up to <code class="docutils literal notranslate"><span class="pre">input_file_50</span></code>.</p>
<p>As an a second example, consider compressing a folder of 50 <code class="docutils literal notranslate"><span class="pre">.fasta</span></code> files. We could just run <code class="docutils literal notranslate"><span class="pre">pigz</span> <span class="pre">*.fasta</span></code>, but each file will be processed sequentially - and where’s the fun in that when you have a cluster with thousands of CPUs? Instead, a simple array job can run this in parallel and compress all 50 files <em>at the same time</em>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>#!/bin/bash

#SBATCH --array=0-49

FILES=(*.fasta)
pigz ${FILES[$SLURM_ARRAY_TASK_ID]}
</pre></div>
</div>
<p>We’ve done two things here:</p>
<ul class="simple">
<li><p>retrieved a list of the <code class="docutils literal notranslate"><span class="pre">.fasta</span></code> files in the current directory and stored them in a Bash array variable called <code class="docutils literal notranslate"><span class="pre">FILES</span></code></p></li>
<li><p>run <code class="docutils literal notranslate"><span class="pre">pigz</span></code> on each element within that array (<code class="docutils literal notranslate"><span class="pre">${FILES[0]}</span></code>, <code class="docutils literal notranslate"><span class="pre">${FILES[1]}</span></code>, etc)</p></li>
</ul>
<p>Note that because Bash arrays are zero-indexed, we therefore told Slurm to run from 0-49 (rather than 1-50) to deal with this. Array jobs also produce a separate <code class="docutils literal notranslate"><span class="pre">.out</span></code> file for each task, so if this job had an ID of 25000, we’d have created output files called <code class="docutils literal notranslate"><span class="pre">slurm-25000_0.out</span></code>, <code class="docutils literal notranslate"><span class="pre">slurm-25000_1.out</span></code> and so on.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Each job submitted to the cluster requires a certain amount of resources, so if you’ve got a large number of jobs that only differ from each other in a minor way, and it’s possible to distinguish between them using variables like <code class="docutils literal notranslate"><span class="pre">$SLURM_ARRAY_TASK_ID</span></code>, then it’s <strong>much</strong> more efficient in terms of resources and Slurm job scheduling to submit a single array job with many tasks rather than many individual jobs.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you have an array job with <em>a lot</em> of sub tasks, you can limit the maximum number of tasks running at the same time by using a <code class="docutils literal notranslate"><span class="pre">%</span></code> separator, eg, <code class="docutils literal notranslate"><span class="pre">--array=1-100000%250</span></code> - in this case limiting the job to 250 simultaneously running tasks.</p>
</div>
</section>
<section id="id1">
<h2>Allocating resources<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<section id="queues-cpus-and-memory">
<h3>Queues, CPUs, and memory<a class="headerlink" href="#queues-cpus-and-memory" title="Permalink to this headline"></a></h3>
<p>Each job task is assigned 1 CPU and 1 GB of memory by default, and is submitted to the <code class="docutils literal notranslate"><span class="pre">medium</span></code> queue, but you can request different resources by passing additional parameters to Slurm.</p>
<p>For instance, to start an interactive job on the <code class="docutils literal notranslate"><span class="pre">short</span></code> queue with 8 CPUs and 16 GB of memory, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ srun --partition=short --cpus-per-task=8 --mem=16G --pty bash
</pre></div>
</div>
<p>Or to provide the same options in an <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> script, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --partition=short</span>
<span class="c1">#SBATCH --cpus-per-task=8</span>
<span class="c1">#SBATCH --mem=16G</span>

<span class="n">echo</span> <span class="s2">&quot;CPUs available: $SLURM_CPUS_PER_TASK&quot;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you don’t know what resources your job needs, it may be tempting to ask for more CPUs or memory than required - just to be safe - but you also need to be sensible with your requests, as over-allocation of resources will lower cluster availability, negativily impacting everyone. There’s much more discussion of this on the <a class="reference internal" href="slurm-policy.html"><span class="doc">Slurm - Queue Policies &amp; Advice</span></a> page.</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>All our servers have hyperthreading meaning each core can run two threads at once. When you request a certain number of “CPUs” from SLURM you are requesting threads (not cores). However SLURM cannot make two different jobs share the threads of a single core, so two different jobs or job tasks will not share a physical core. This means, for example, that a job requesting three CPUs will actually be allocated two full physical cores (four threads), but still only have use of three.</p>
<p>You’re therefore better off submitting jobs that always ask for an even number of CPUs.</p>
</div>
<p>Further to the above point, you can see this hyperthread allocation in action by starting a simple interactive job and querying the node info:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ srun --pty bash
$ sinfo -N -o &quot;%25N %9R %14C&quot;
NODELIST                  PARTITION CPUS(A/I/O/T)
n19-32-192-hulk           medium    2/64/0/64
</pre></div>
</div>
<p>We passed no extra parameters, meaning the job only has access to a single CPU, but it’s actually taken up two CPUs (<code class="docutils literal notranslate"><span class="pre">2/64</span></code>) in the allocation list for the node it’s running on.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>CPU resources may be shared (up to two jobs running per CPU) when a queue is full, but memory is always exclusive, so if you ask for 4 GB then no-one else can use up that memory and negatively effect your job.</p>
</div>
</section>
<section id="gpu-resources">
<h3>GPU resources<a class="headerlink" href="#gpu-resources" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">gpu</span></code> queue must be used to access a GPU, which are available on the <code class="docutils literal notranslate"><span class="pre">thanos</span></code> node (and later on <code class="docutils literal notranslate"><span class="pre">jaws</span></code> once setup). Select the <code class="docutils literal notranslate"><span class="pre">gpu</span></code> queue and use the –gpus option to request one or both of the available GPUs. See <a class="reference internal" href="gpu.html"><span class="doc">GPU Processing</span></a> for details.</p>
</section>
</section>
<section id="cancelling-a-job">
<h2>Cancelling a job<a class="headerlink" href="#cancelling-a-job" title="Permalink to this headline"></a></h2>
<p>To cancel one of your jobs from the queue use <code class="docutils literal notranslate"><span class="pre">scancel</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scancel &lt;jobid&gt;
</pre></div>
</div>
<p>replacing <code class="docutils literal notranslate"><span class="pre">&lt;jobid&gt;</span></code> with the ID of your job.</p>
<p>You can also delete all of your jobs at once:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scancel -u &lt;username&gt;
</pre></div>
</div>
</section>
<section id="scratch-space">
<h2>Scratch space<a class="headerlink" href="#scratch-space" title="Permalink to this headline"></a></h2>
<p>You should ensure your jobs <strong>only</strong> write to scratch space while running. Don’t move any final data to <code class="docutils literal notranslate"><span class="pre">/mnt/shared/projects</span></code> until you’re sastified with the results and ready to back them up.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>Never</strong> write temporary/intermediate working data to a backed up area.</p>
</div>
<p>The cluster recognises two scratch areas:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> - located on the networked BeeGFS system and good for parallel access to large data files. Visible to <code class="docutils literal notranslate"><span class="pre">gruffalo</span></code> and all the compute nodes at all times.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">$TMPDIR</span></code> - located on the local SSD drives installed in each node and good for tasks that require high performance with many small files. Automatically created and destroyed at the beginning and end of each job task and only visible to the node running that task.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <a class="reference internal" href="data-storage.html"><span class="doc">Data Storage &amp; Management Policy</span></a> and <a class="reference internal" href="slurm-policy.html"><span class="doc">Slurm - Queue Policies &amp; Advice</span></a> pages both cover various pros and cons of these two options in more detail.</p>
</div>
</section>
<section id="job-summaries">
<h2>Job summaries<a class="headerlink" href="#job-summaries" title="Permalink to this headline"></a></h2>
<p>You can retrieve summary information about a finished job by using the <code class="docutils literal notranslate"><span class="pre">sacct</span></code> command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sacct -j &lt;jobid&gt;
</pre></div>
</div>
<p>By default this only shows basic information, such as the queue that ran the job and whether it completed or not. For more details try:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sacct -j &lt;jobid&gt; --units=G --format JobID,MaxVMSize,MaxRSS,NodeList,AllocCPUS,TotalCPU,State,Start,End
</pre></div>
</div>
<p>which provides information on:</p>
<ul class="simple">
<li><p><strong>JobID</strong> - the ID of the job</p></li>
<li><p><strong>MaxVMSize</strong> - how much memory the job requested, but did not necessarily fill up (including any swap usage)</p></li>
<li><p><strong>MaxRSS</strong> - the maximum real memory used by the job</p></li>
<li><p><strong>NodeList</strong> - the compute node that ran the job</p></li>
<li><p><strong>AllocCPUS</strong> - how many CPUs were allocated</p></li>
<li><p><strong>TotalCPU</strong> - the total CPU time used by the job, which will often be less than the runtime, especially if the job spent time waiting on user interaction or disk I/O</p></li>
<li><p><strong>State</strong> - the job’s exit state (failed or completed, etc)</p></li>
<li><p><strong>Start</strong> - start time of the job</p></li>
<li><p><strong>End</strong> - end time of the job</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We’ve set up some <a class="reference internal" href="slurm-shortcuts.html"><span class="doc">Slurm - Shortcuts and Aliases</span></a> to make running some of these longer commands easier for you.</p>
</div>
</section>
<section id="other-helpful-parameters">
<h2>Other helpful parameters<a class="headerlink" href="#other-helpful-parameters" title="Permalink to this headline"></a></h2>
<p>The following is a short list of Slurm parameters that you may find helpful. These options can either be given on the command line alongside <code class="docutils literal notranslate"><span class="pre">srun</span></code> and <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> or inside a job’s script file using <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>. You’ll find lots more by running <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">sbatch</span></code>.</p>
<section id="change-working-directory">
<h3>Change working directory<a class="headerlink" href="#change-working-directory" title="Permalink to this headline"></a></h3>
<p>By default, Slurm’s working directory is the same as the one you used to submit the job. Any paths in your script will be relative to this location, and the <code class="docutils literal notranslate"><span class="pre">.out</span></code> files will be written here too, but you can override this using <code class="docutils literal notranslate"><span class="pre">chdir</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --chdir=&lt;directory&gt;</span>
</pre></div>
</div>
</section>
<section id="email-notifications">
<h3>Email notifications<a class="headerlink" href="#email-notifications" title="Permalink to this headline"></a></h3>
<p>You can have Slurm send you emails at various stages of a job’s life, for example, to be notified when a job successfully completes or has failed, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --mail-user=email@address.com</span>
<span class="c1">#SBATCH --mail-type=END,FAIL</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>While you can be notified separately for <em>each</em> task of an array job by using <code class="docutils literal notranslate"><span class="pre">--mail-type=END,FAIL,ARRAY_TASKS</span></code> you should be very careful of doing this with large array sizes or you’ll swamp yourself with hundreds or thousands of emails!</p>
</div>
</section>
<section id="job-testing">
<h3>Job testing<a class="headerlink" href="#job-testing" title="Permalink to this headline"></a></h3>
<p>If you pass <code class="docutils literal notranslate"><span class="pre">--test-only</span></code> as a parameter to <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>, it’ll validate your batch script and give you an estimate of when your job will start. You can then tweak any requested resources (number of CPUs, amount of memory, etc) and try again, potentially enabling your job to start sooner.</p>
</section>
<section id="specifying-nodes">
<h3>Specifying nodes<a class="headerlink" href="#specifying-nodes" title="Permalink to this headline"></a></h3>
<p>You can request a specific node to run your job (within the confines of the queue you’ve asked for), by using <code class="docutils literal notranslate"><span class="pre">--nodelist</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --nodelist=n19-32-192-loki</span>
</pre></div>
</div>
<p>You can specify more than one node in the list, by separating the node names with commas, but note this will request allocation of all those nodes at once rather than select one from the list. You can also do the reverse and exclude node(s) by using <code class="docutils literal notranslate"><span class="pre">--exclude</span></code>.</p>
</section>
<section id="submitting-binaries">
<h3>Submitting binaries<a class="headerlink" href="#submitting-binaries" title="Permalink to this headline"></a></h3>
<p>If you want to run a simple binary command, it can be quicker to use the <code class="docutils literal notranslate"><span class="pre">--wrap</span></code> option, rather than creating a script, for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ sbatch --wrap &quot;pigz hugefile.txt&quot;
</pre></div>
</div>
<p>Slurm now does the work of wrapping that up into a (virtual) script for you and submits it to the queue.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="guest-accounts.html" class="btn btn-neutral float-left" title="Guest Accounts" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="slurm-policy.html" class="btn btn-neutral float-right" title="Slurm - Queue Policies &amp; Advice" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 Information &amp; Computational Sciences, The James Hutton Institute.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>