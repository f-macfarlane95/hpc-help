<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Slurm - Queue Policies &amp; Advice &mdash; Crop Diversity HPC Help  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Slurm - Shortcuts and Aliases" href="slurm-shortcuts.html" />
    <link rel="prev" title="Slurm - Overview" href="slurm-overview.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Crop Diversity HPC Help
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="system-overview.html">System Overview</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="user-accounts.html">Requesting an Account</a></li>
<li class="toctree-l1"><a class="reference internal" href="ssh.html">Getting Connected</a></li>
<li class="toctree-l1"><a class="reference internal" href="guest-accounts.html">Guest Accounts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">High performance computing</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="slurm-overview.html">Slurm - Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Slurm - Queue Policies &amp; Advice</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#specifying-queues">Specifying queues</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fair-share-policy">Fair share policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="#additional-advice-and-guidance">Additional advice and guidance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#can-i-use-the-entire-cluster-at-once">Can I use the entire cluster at once?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#which-queue-partition-should-i-use">Which queue/partition should I use?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#where-should-i-write-data-to">Where should I write data to?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-much-cpu-memory-should-i-allocate-to-a-job">How much CPU/memory should I allocate to a job?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="slurm-shortcuts.html">Slurm - Shortcuts and Aliases</a></li>
<li class="toctree-l1"><a class="reference internal" href="bioconda.html">Bioconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiling.html">Compiling Software</a></li>
<li class="toctree-l1"><a class="reference internal" href="file-compression.html">Working with Compressed Files</a></li>
<li class="toctree-l1"><a class="reference internal" href="singularity.html">Apptainer (Singularity)</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu.html">GPU Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="machine-learning.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="apps.html">Tools &amp; Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="database-mirrors.html">Database Mirrors</a></li>
<li class="toctree-l1"><a class="reference internal" href="openmpi.html">OpenMPI</a></li>
<li class="toctree-l1"><a class="reference internal" href="green-computing.html">Green Computing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data storage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="data-storage.html">Data Storage &amp; Management Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="backups.html">Backups</a></li>
<li class="toctree-l1"><a class="reference internal" href="file-transfers.html">File Transfers</a></li>
<li class="toctree-l1"><a class="reference internal" href="samba.html">Samba Access</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Miscellaneous</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contact-us.html">Contact Us</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">Monitoring &amp; Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="linux-basics.html">Linux Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="tips.html">Tips &amp; Tricks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">The legal stuff</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="organizations.html">Supported Organisations</a></li>
<li class="toctree-l1"><a class="reference internal" href="sla.html">Service Level Agreement</a></li>
<li class="toctree-l1"><a class="reference internal" href="acceptable-use.html">Acceptable Use Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="gdpr.html">GDPR Privacy Notice</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Crop Diversity HPC Help</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Slurm - Queue Policies &amp; Advice</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/cropgeeks/hpc-help/blob/master/slurm-policy.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="slurm-queue-policies-advice">
<h1>Slurm - Queue Policies &amp; Advice<a class="headerlink" href="#slurm-queue-policies-advice" title="Permalink to this headline"></a></h1>
<p>Our Slurm setup runs with the following goals and constraints in mind:</p>
<ul class="simple">
<li><p>allow short jobs to run without having to wait more than a few hours</p></li>
<li><p>do not permit many long jobs to take over the entire cluster for long periods</p></li>
<li><p>try to divide the cluster equally among users</p></li>
<li><p>keep all of the cluster’s processors as busy as possible all of the time</p></li>
</ul>
<p>To do this, we primarily use three main queues/partitions called <code class="docutils literal notranslate"><span class="pre">short</span></code>, <code class="docutils literal notranslate"><span class="pre">medium</span></code> and <code class="docutils literal notranslate"><span class="pre">long</span></code> (referring to their runtime), with <code class="docutils literal notranslate"><span class="pre">medium</span></code> being the default queue that jobs will go to unless you specify otherwise:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Queue</p></th>
<th class="head"><p>CPUs</p></th>
<th class="head"><p>Max RAM</p></th>
<th class="head"><p>Time Limit</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">short</span></code></p></td>
<td><p>384</p></td>
<td><p>192 GB</p></td>
<td><p>6 hours</p></td>
<td><p>This is a high priority queue for smaller jobs with thresholds set to allow smaller jobs to squeeze through that might have to wait in the other queues</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">medium</span></code></p></td>
<td><p>768</p></td>
<td><p>192 GB</p></td>
<td><p>24 hours</p></td>
<td><p>This is the default queue that all jobs will submit to unless otherwise requested</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">long</span></code></p></td>
<td><p>2,032</p></td>
<td><p>192 - 384 GB</p></td>
<td><p>No limit</p></td>
<td><p>This queue is for long running jobs</p></td>
</tr>
</tbody>
</table>
<p>There are also two special queues that should only be used for jobs that require large amounts of memory or access to <a class="reference internal" href="gpu.html"><span class="doc">GPU Processing</span></a>:</p>
<table class="colwidths-given docutils align-default">
<colgroup>
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Queue</p></th>
<th class="head"><p>CPUs</p></th>
<th class="head"><p>Max RAM</p></th>
<th class="head"><p>Time Limit</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">himem</span></code></p></td>
<td><p>296</p></td>
<td><p>1.5 - 3.0 TB</p></td>
<td><p>No limit</p></td>
<td><p>This queue is for jobs requiring a very large amount of RAM</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">gpu</span></code></p></td>
<td><p>48</p></td>
<td><p>70 GB</p></td>
<td><p>No limit</p></td>
<td><p>This queue is for jobs requiring GPUs</p></td>
</tr>
</tbody>
</table>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If you have a job that will last more than 21 days, we ask that you <a class="reference internal" href="contact-us.html"><span class="doc">Contact Us</span></a> beforehand to discuss its requirements. We reserve the right to <strong>terminate any long running job</strong> that we believe is negatively affecting the cluster, running inefficiently, and/or impacting other users.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All queues run with the same priority across all nodes. Only the time limits differ, with the <code class="docutils literal notranslate"><span class="pre">short</span></code> and <code class="docutils literal notranslate"><span class="pre">medium</span></code> queues automatically killing a job if it exceeds their limits. GPUs can only be accessed from the <code class="docutils literal notranslate"><span class="pre">gpu</span></code> queue, and large RAM requests can only run on the <code class="docutils literal notranslate"><span class="pre">himem</span></code> queue.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The maximum amount of memory you can request (per node) will be a few GB less than shown above, because Slurm reserves some allocation for the rest of the operating system.</p>
</div>
<section id="specifying-queues">
<h2>Specifying queues<a class="headerlink" href="#specifying-queues" title="Permalink to this headline"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>No special options are required to submit to the <code class="docutils literal notranslate"><span class="pre">medium</span></code> queue.</p>
</div>
<p>To submit to the <code class="docutils literal notranslate"><span class="pre">short</span></code> queue, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="o">--</span><span class="n">partition</span><span class="o">=</span><span class="n">short</span> <span class="n">myscript</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>Or to submit to the <code class="docutils literal notranslate"><span class="pre">long</span></code> queue, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="o">--</span><span class="n">partition</span><span class="o">=</span><span class="n">long</span> <span class="n">myscript</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>To submit to the high memory (<code class="docutils literal notranslate"><span class="pre">himem</span></code>) queue, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="o">--</span><span class="n">partition</span><span class="o">=</span><span class="n">himem</span> <span class="n">myscript</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>To submit to the <code class="docutils literal notranslate"><span class="pre">gpu</span></code> queue, where <code class="docutils literal notranslate"><span class="pre">n</span></code> specifies how many GPUs you want to use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span> <span class="o">--</span><span class="n">partition</span><span class="o">=</span><span class="n">gpu</span> <span class="o">--</span><span class="n">gpus</span><span class="o">=</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="n">myscript</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>For more details on accessing the GPUs, see <a class="reference internal" href="gpu.html"><span class="doc">GPU Processing</span></a>.</p>
<p>To get a job list for an individual queue rather than all queues, use the <code class="docutils literal notranslate"><span class="pre">-p</span></code> or <code class="docutils literal notranslate"><span class="pre">--partition</span></code> option for <code class="docutils literal notranslate"><span class="pre">squeue</span></code>, for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">squeue</span> <span class="o">--</span><span class="n">partition</span><span class="o">=</span><span class="n">short</span>
</pre></div>
</div>
</section>
<section id="fair-share-policy">
<h2>Fair share policy<a class="headerlink" href="#fair-share-policy" title="Permalink to this headline"></a></h2>
<p>The cluster uses a fair share policy that is applied according to computational time (ie time is allocated fairly with all users having equal share).</p>
<p>When you submit your first job it’ll go to the end of the queue, but the scheduler will quickly move it higher up the queue if other users with jobs running also have waiting jobs ahead of you in that queue. This is because Slurm attempts to divide the available CPUs equally among all users (our fair-share policy). For example, if user A has multiple jobs queued and running, and then user B queues new jobs, those new jobs will rise in priority above some of A’s jobs until the number of running jobs is approximately shared equally between the two users (although B’s jobs may still have to wait until some of the previous jobs finish).</p>
<p>These rules apply to both interactive and sbatch jobs.</p>
</section>
<section id="additional-advice-and-guidance">
<h2>Additional advice and guidance<a class="headerlink" href="#additional-advice-and-guidance" title="Permalink to this headline"></a></h2>
<p>Below are some additional questions you may have about using the cluster in a sensible - and fair - manner. Don’t hesitate to <a class="reference internal" href="contact-us.html"><span class="doc">Contact Us</span></a> if you’re unsure though.</p>
<section id="can-i-use-the-entire-cluster-at-once">
<h3>Can I use the entire cluster at once?<a class="headerlink" href="#can-i-use-the-entire-cluster-at-once" title="Permalink to this headline"></a></h3>
<p>It depends.</p>
<p>While there are currently no limits to prevent you from submitting a job that uses every CPU across one or more queues, you first need to ask yourself how sensible that would be? Consider:</p>
<ul class="simple">
<li><p>how long the job will last? Short running tasks allow others’ jobs to rise in priority above yours (the fair-share policy), so submitting a 10,000 jobs that only last a few minutes each will ‘hog’ the cluster much less than just a few tens or hundreds of jobs that last for hours and hours.</p></li>
<li><p>how busy is the cluster? If it’s 2am and no-one else is using the cluster, then it’s less likely to be detrimental to anyone else.</p></li>
<li><p>how much you value your friendship with other cluster users? Seriously. This is a shared resource, and while it’s here to be used, it’s not here to be abused.</p></li>
</ul>
</section>
<section id="which-queue-partition-should-i-use">
<h3>Which queue/partition should I use?<a class="headerlink" href="#which-queue-partition-should-i-use" title="Permalink to this headline"></a></h3>
<p>It depends.</p>
<p>Based purely on historical observation and anecdotal evidence, a significant number of jobs seem to complete OK within 24 hours (so the default medium queue is probably fine), but obviously the bigger your job or data sets that you want to process, the more likely it is to overrun and therefore be safer on the long queue. However, if the long queue is busy, you may then have to wait longer for your job to start. Note though, that each task of an array job has its own time allocation, so you could still successfully run a week-long job on the medium queue if each of its subtasks completes in less than 24 hours.</p>
<p>If it’s an interactive job, then you’re probably better off running it on the short queue.</p>
</section>
<section id="where-should-i-write-data-to">
<h3>Where should I write data to?<a class="headerlink" href="#where-should-i-write-data-to" title="Permalink to this headline"></a></h3>
<p>It depends.</p>
<p>During a job, you should almost always be writing output data to one of the scratch locations, however there’s a choice of storage locations each with their own pros and cons:</p>
<p>Shared network <strong>BeeGFS scratch space</strong> (<code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> or <code class="docutils literal notranslate"><span class="pre">/mnt/shared/scratch/$USER</span></code>) is accessible from any node and may be where your data is already residing. It’s a parallel storage array and reasonably fast when dealing with very large sequential reads or writes - so great for stream reading from multiple large .bam files for instance - but not so good if your job has to read or write hundreds or millions of very tiny files. As part of the main storage array it also has plenty of free space.</p>
<p><strong>Node-specific scratch space</strong> (<code class="docutils literal notranslate"><span class="pre">$TMPDIR</span></code>) is local to each node and uses an array of SSDs for performance so it can be much faster than BeeGFS for certain use cases, but each node’s capacity is limited (see <a class="reference internal" href="system-overview.html"><span class="doc">System Overview</span></a> for details) and you need to copy your data there first.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">$TMPDIR</span></code> is automatically created - and destroyed! - as part of a job submission, so it’s up to you to copy any input data here as the first step of an sbatch submission, and to copy data out again at the end.</p>
</div>
</section>
<section id="how-much-cpu-memory-should-i-allocate-to-a-job">
<h3>How much CPU/memory should I allocate to a job?<a class="headerlink" href="#how-much-cpu-memory-should-i-allocate-to-a-job" title="Permalink to this headline"></a></h3>
<p>It depends.</p>
<p>Although <code class="docutils literal notranslate"><span class="pre">gruffalo</span></code> can automatically manage and prioritise jobs well - most of the time - you still need to ensure sensible job-allocation requests are made.</p>
<p>Try to avoid submitting jobs that lock out too much of the cluster at once, either by using too many CPUs simultaneously for an excessive amount of time, or by requesting resources far beyond those actually used (eg asking for 16 CPUs for a process that only uses one for the majority of its runtime, or 100 GB of memory for a job that only uses a fraction of that). Over-allocation of resources negatively affects both other users and additional jobs of your own.</p>
<p>However, if you under-allocate on memory, the cluster will kill jobs that try to go beyond their requested allocation. It may therefore be tempting to just over-allocate everything for every job, asking for all the CPUs or all the memory, but this is easily spotted and we’ll take action if we notice your jobs continually requesting resources significantly beyond what they’re using. Jobs requesting more resources also tend to take longer to run as they must wait until all those resources become available if the cluster is busy. It may just take a little trial and error until you get confortable with how much to request for a given job or data set.</p>
<p>Finally, you should also take <a class="reference internal" href="green-computing.html"><span class="doc">Green Computing</span></a> into account. A single node running 32 tasks uses <strong>far less energy</strong> than 32 nodes running 1 task each. If you over-allocate resources, then more nodes need to be online to meet your requirements, which wastes power if they’re not being used effectively.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="slurm-overview.html" class="btn btn-neutral float-left" title="Slurm - Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="slurm-shortcuts.html" class="btn btn-neutral float-right" title="Slurm - Shortcuts and Aliases" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022 Information &amp; Computational Sciences, The James Hutton Institute.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>